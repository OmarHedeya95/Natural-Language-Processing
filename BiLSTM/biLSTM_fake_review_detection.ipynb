{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0tH2rdOL6Fn"
      },
      "source": [
        "# **Import the packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0RHJvElArxd",
        "outputId": "49274863-9281-4150-9d92-08c127587560"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=set(stopwords.words('english'))\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout,Bidirectional, Input, concatenate\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsVzV8WZMOKI"
      },
      "source": [
        "# **Data Analysis and pre-processing**\n",
        "*Assumption:* The file 'amazon_reviews.txt' need to be uploaded to google colab.\n",
        "We use pandas to read the data. Our text file is 'tab' delimited. Hence we use *delimiter=\\t* in the read_csv() of pandas. Some rows of the data has missing columns. We want to remove these columns during preprocessing. Hence we have the argument *error_bad_lines=False* which will make sure some of the rows with missing values will not be read. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saMGg92oHw-D",
        "outputId": "cf482e59-239f-4d9b-ae7d-4e0dbaba55cf"
      },
      "source": [
        "dataset = pd.read_csv('amazon_reviews.txt', delimiter='\\t', error_bad_lines=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 6: expected 9 fields, saw 16\\nSkipping line 21006: expected 9 fields, saw 10\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XTNPuOONxOu"
      },
      "source": [
        "**Analyse the data** <br/>\n",
        "We use the head() of the pandas package to see what our data looks like. From this data we can see that 'LABEL' is the column we are trying to predict. To predict this value we make use of different features from the data. \n",
        "The different features we are using are 'RATING', 'REVIEW_TEXT' and 'VERIFIED_PURCHASE'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "bjEDkr7xII24",
        "outputId": "933b4bac-66e5-4ffc-85fd-a5bfa6b0881e"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>LABEL</th>\n",
              "      <th>RATING</th>\n",
              "      <th>VERIFIED_PURCHASE</th>\n",
              "      <th>PRODUCT_CATEGORY</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>PRODUCT_TITLE</th>\n",
              "      <th>REVIEW_TITLE</th>\n",
              "      <th>REVIEW_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>__label1__</td>\n",
              "      <td>4</td>\n",
              "      <td>N</td>\n",
              "      <td>PC</td>\n",
              "      <td>B00008NG7N</td>\n",
              "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
              "      <td>useful</td>\n",
              "      <td>When least you think so, this product will sav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>__label1__</td>\n",
              "      <td>4</td>\n",
              "      <td>Y</td>\n",
              "      <td>Wireless</td>\n",
              "      <td>B00LH0Y3NM</td>\n",
              "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
              "      <td>New era for batteries</td>\n",
              "      <td>Lithium batteries are something new introduced...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>__label1__</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>Baby</td>\n",
              "      <td>B000I5UZ1Q</td>\n",
              "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
              "      <td>doesn't swing very well.</td>\n",
              "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>__label1__</td>\n",
              "      <td>4</td>\n",
              "      <td>N</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>B003822IRA</td>\n",
              "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
              "      <td>Great computing!</td>\n",
              "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>__label1__</td>\n",
              "      <td>4</td>\n",
              "      <td>N</td>\n",
              "      <td>PC</td>\n",
              "      <td>B00008NG7N</td>\n",
              "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
              "      <td>useful</td>\n",
              "      <td>When least you think so, this product will sav...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DOC_ID  ...                                        REVIEW_TEXT\n",
              "0       1  ...  When least you think so, this product will sav...\n",
              "1       2  ...  Lithium batteries are something new introduced...\n",
              "2       3  ...  I purchased this swing for my baby. She is 6 m...\n",
              "3       4  ...  I was looking for an inexpensive desk calcolat...\n",
              "4       1  ...  When least you think so, this product will sav...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os_kvcCcOvkS"
      },
      "source": [
        "We observe that the 'LABEL' and 'VERIFIED_PURCHASE' column are Categorical. To work with Deep learning models we need to convert these Categorical values to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6bYN4WIwox"
      },
      "source": [
        "dataset['LABEL'] = pd.Categorical(dataset['LABEL'])\n",
        "dataset['LABEL'] = dataset['LABEL'].cat.codes"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgub9D_WBCSd"
      },
      "source": [
        "dataset['VERIFIED_PURCHASE'] = pd.Categorical(dataset['VERIFIED_PURCHASE'])\n",
        "dataset['VERIFIED_PURCHASE'] = dataset['VERIFIED_PURCHASE'].cat.codes"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaDbMbrzPK59"
      },
      "source": [
        "# **Train, validation and test split**\n",
        "We have the data to work with and we need to split it to training set, validation set and test set. We train our model on the training set, tune our hyperparameters on the validation set and finally use it on the test set. We split the data into 80% training set, 10% validation set and 10% test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stT4HNk_ENlM"
      },
      "source": [
        "x_train, x_testval, y_train, y_testval = train_test_split(dataset[['REVIEW_TEXT','RATING','VERIFIED_PURCHASE']], dataset[['LABEL']] , test_size = 0.20)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_testval[['REVIEW_TEXT','RATING','VERIFIED_PURCHASE']], y_testval[['LABEL']] , test_size = 0.50)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIbDoCQWFXT3"
      },
      "source": [
        "x_train_review = x_train['REVIEW_TEXT']\n",
        "x_train_verified_purchase = x_train['VERIFIED_PURCHASE']\n",
        "x_train_rating = x_train['RATING']\n",
        "x_val_review = x_val['REVIEW_TEXT']\n",
        "x_val_verified_purchase = x_val['VERIFIED_PURCHASE']\n",
        "x_val_rating = x_val['RATING']\n",
        "x_test_review = x_test['REVIEW_TEXT']\n",
        "x_test_verified_purchase = x_test['VERIFIED_PURCHASE']\n",
        "x_test_rating = x_test['RATING']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIYuEtXWQNAu"
      },
      "source": [
        "# **Tokenize and Build vocabulary**\n",
        "Once we have our training data we tokenize the 'REVIEW_TEXT' part of the training data and build a vocabulary from that. We have defined the vocabulary size to be 20000. Any token which is not part of the vocabulary will be repalced with \\<unk>. We also set the max length to 100 which means that any sentence have a length greater than 100 will be truncated and any sentence having a length less than 100 will be padded.\n",
        "We then convert the sequence of texts into sequence of integers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPFo6EdNJaIc"
      },
      "source": [
        "vocab_size = 20000\n",
        "oov_token = \"<OOV>\"\n",
        "max_length = 100\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG-O6hgVJeft"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train_review)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34jhDJX5Jkzf"
      },
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(x_train_review)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCDttH12JoDA"
      },
      "source": [
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
        "                       truncating=trunction_type)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnEY5ChvQQ32"
      },
      "source": [
        "X_val_sequences = tokenizer.texts_to_sequences(x_val_review)\n",
        "X_val_padded = pad_sequences(X_val_sequences,maxlen=max_length, \n",
        "                               padding=padding_type, truncating=trunction_type)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s98QfTnjJtLT"
      },
      "source": [
        "X_test_sequences = tokenizer.texts_to_sequences(x_test_review)\n",
        "X_test_padded = pad_sequences(X_test_sequences,maxlen=max_length, \n",
        "                               padding=padding_type, truncating=trunction_type)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dol4lag7Rbkh"
      },
      "source": [
        "# **Transfer Learning: Use pre-trained word embeddings**\n",
        "In natural language processing, each word is represented as a dense vector. This kind of representation allows the model to understand what a word actually means. For this dense vector representation we use pre-trained GloVe embeddings.In our case we decided to represent each word with a 100-dimensional vector.\n",
        "\n",
        "Below we download the GloVe embeddings, and create a embedding matrix from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHQTHSfoJ6xq",
        "outputId": "e7730219-cd4c-43e5-ec33-b101efb47ef7"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-04 17:49:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-04 17:49:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-04 17:49:34--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 40s  \n",
            "\n",
            "2021-08-04 17:52:15 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GnScyADKntf",
        "outputId": "56048b18-9d16-4d01-c850-734804693f34"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv8FnbRpKxCk",
        "outputId": "bb01bf97-a716-4c90-9f8e-b040e19014f0"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMb5i1kxLIlz"
      },
      "source": [
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, max_length))\n",
        "#print(embedding_matrix.shape)\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        #print(embedding_vector.shape)\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baZCY1BVLQ5B"
      },
      "source": [
        "#https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_G94fWxTDzf"
      },
      "source": [
        "# **Build the model**\n",
        "The model that we have build has 3 heads. Each of the head will process 3 different features. The results of the 3 heads are concatenated and processed further in the feed forward network to get the final predictions. After a lot of experimentation on how to process these 3 features seperately we finalized this architecture. \n",
        "\n",
        "\n",
        "\n",
        "1.   First head is the **REVIEW_TEXT head**. This part of our model processes only the review text. We have a initial embedding layer which provides the embeddings for all the tokens in the training data. We process the sequence using a **Bi directional LSTM**. The output from bidirectional LSTM is fed to a dense Fully coonected layer\n",
        "2.   Second head is the **RATING head** which is used to process the RATING feature in a dence fully connected layer.\n",
        "3. Third head is the **VERIFIED_PURCHASE head** which will take as input the VERIFIED_PURCHASE feature and produce a intermediate represenataion for it.\n",
        "\n",
        "The output from the 3 heads are concatenated and passed to a fully connected layer which does the predection. \n",
        "We use a 'Binary Cross Entropy' loss and all the 3 heads are trained jointly.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Z3a3YYLWWM"
      },
      "source": [
        "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
        "                            max_length,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            #input_length=max_length,\n",
        "                            trainable=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehCoOGCxKi6l"
      },
      "source": [
        "from keras.models import Model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOEnoSJyIrSJ"
      },
      "source": [
        "review_branch_ip = Input(shape=(100,), name='Review_input')\n",
        "review_branch = embedding_layer(review_branch_ip)\n",
        "review_branch = Dropout(0.2)(review_branch)\n",
        "review_branch = Bidirectional(\n",
        "    LSTM(64, dropout=0.2,recurrent_dropout=0.5)\n",
        ")(review_branch)\n",
        "review_branch = Dense(64,activation='relu')(review_branch)\n",
        "review_branch_op = Dense(16, activation='relu')(review_branch)\n",
        "#output = Dense(1,activation='sigmoid')(review_branch)\n",
        "\n",
        "\n",
        "rating_branch_ip = Input(shape=(1,), name='Rating_input')\n",
        "rating_branch = Dense(8,activation='relu')(rating_branch_ip)\n",
        "rating_branch = Dropout(0.2)(rating_branch)\n",
        "rating_branch_op = Dense(16,activation='relu')(rating_branch)\n",
        "\n",
        "\n",
        "\n",
        "verified_purchase_branch_ip = Input(shape=(1,), name='Verified_Purchase_input')\n",
        "verified_purchase_branch = Dense(8,activation='relu')(verified_purchase_branch_ip)\n",
        "verified_purchase_branch = Dropout(0.2)(verified_purchase_branch)\n",
        "verified_purchase_branch_op = Dense(16,activation='relu')(verified_purchase_branch)\n",
        "\n",
        "\n",
        "concat = concatenate([review_branch_op, rating_branch_op, verified_purchase_branch_op], name='Concatenate')\n",
        "\n",
        "\n",
        "final_op = Dense(8, activation='relu')(concat)\n",
        "final_output = Dense(1,activation='sigmoid')(final_op)\n",
        "\n",
        "model = Model(inputs=[review_branch_ip,rating_branch_ip,verified_purchase_branch_ip], outputs=final_output,\n",
        "                    name='Final_output')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0alr3QVYdI"
      },
      "source": [
        "The model is compiled using the ADAM optimizer and Binary Cross Entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reKaMzr5MXAC"
      },
      "source": [
        "optimzer = Adam(clipvalue=0.5) # clip value to avoid the gradient exploding\n",
        "\n",
        "model.compile(optimizer=optimzer, \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYC5oTzQMc-i",
        "outputId": "14d2634c-309f-4778-defb-816ce7fd02f6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Final_output\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Review_input (InputLayer)       [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 100)     3589800     Review_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100, 100)     0           embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Rating_input (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Verified_Purchase_input (InputL [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 128)          84480       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 8)            16          Rating_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 8)            16          Verified_Purchase_input[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           8256        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 8)            0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 8)            0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 16)           1040        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 16)           144         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 16)           144         dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Concatenate (Concatenate)       (None, 48)           0           dense_9[0][0]                    \n",
            "                                                                 dense_11[0][0]                   \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 8)            392         Concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            9           dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,684,297\n",
            "Trainable params: 94,497\n",
            "Non-trainable params: 3,589,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOiJy71RMixh",
        "outputId": "3044fe81-a90a-4e97-8496-0a32371c8918"
      },
      "source": [
        "history = model.fit([X_train_padded,x_train_rating,x_train_verified_purchase], y_train, batch_size=32,epochs=10, validation_data=([X_val_padded,x_val_rating,x_val_verified_purchase], y_val))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1050/1050 [==============================] - 291s 268ms/step - loss: 0.5789 - acc: 0.7062 - val_loss: 0.4858 - val_acc: 0.7910\n",
            "Epoch 2/10\n",
            "1050/1050 [==============================] - 278s 264ms/step - loss: 0.4827 - acc: 0.7879 - val_loss: 0.4792 - val_acc: 0.7883\n",
            "Epoch 3/10\n",
            "1050/1050 [==============================] - 279s 266ms/step - loss: 0.4689 - acc: 0.7925 - val_loss: 0.4711 - val_acc: 0.7905\n",
            "Epoch 4/10\n",
            "1050/1050 [==============================] - 280s 267ms/step - loss: 0.4560 - acc: 0.7994 - val_loss: 0.4620 - val_acc: 0.7943\n",
            "Epoch 5/10\n",
            "1050/1050 [==============================] - 278s 265ms/step - loss: 0.4444 - acc: 0.8042 - val_loss: 0.4506 - val_acc: 0.8029\n",
            "Epoch 6/10\n",
            "1050/1050 [==============================] - 279s 266ms/step - loss: 0.4391 - acc: 0.8082 - val_loss: 0.4568 - val_acc: 0.8014\n",
            "Epoch 7/10\n",
            "1050/1050 [==============================] - 279s 266ms/step - loss: 0.4259 - acc: 0.8171 - val_loss: 0.4388 - val_acc: 0.8105\n",
            "Epoch 8/10\n",
            "1050/1050 [==============================] - 280s 266ms/step - loss: 0.4120 - acc: 0.8198 - val_loss: 0.4219 - val_acc: 0.8143\n",
            "Epoch 9/10\n",
            "1050/1050 [==============================] - 278s 265ms/step - loss: 0.4046 - acc: 0.8273 - val_loss: 0.4194 - val_acc: 0.8193\n",
            "Epoch 10/10\n",
            "1050/1050 [==============================] - 280s 266ms/step - loss: 0.3956 - acc: 0.8292 - val_loss: 0.4066 - val_acc: 0.8276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYmJ6_0ioX-"
      },
      "source": [
        "**Evaluate and save the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkOhFOxThW_9",
        "outputId": "e4c7322f-b620-4f46-8a63-e1e56e2b6ab2"
      },
      "source": [
        "model.evaluate([X_test_padded, x_test_rating, x_test_verified_purchase], y_test, verbose=2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132/132 - 4s - loss: 0.4043 - acc: 0.8374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40432077646255493, 0.8373809456825256]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pibiq1jzfPTN"
      },
      "source": [
        "!mkdir -p models\n",
        "model.save('models/blstmv1.h5')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JUm6dijTAU"
      },
      "source": [
        "**Reload the model and verify the accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEzb1HVsizVZ"
      },
      "source": [
        "new_model = tf.keras.models.load_model('models/blstmv1.h5')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z1fV5IxizLD",
        "outputId": "b857b38d-b092-4833-cd24-ae2e5d976d69"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Final_output\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Review_input (InputLayer)       [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 100)     3589800     Review_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100, 100)     0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Rating_input (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Verified_Purchase_input (InputL [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 128)          84480       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 8)            16          Rating_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 8)            16          Verified_Purchase_input[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           8256        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 8)            0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 8)            0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 16)           1040        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 16)           144         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 16)           144         dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Concatenate (Concatenate)       (None, 48)           0           dense_9[0][0]                    \n",
            "                                                                 dense_11[0][0]                   \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 8)            392         Concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            9           dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,684,297\n",
            "Trainable params: 94,497\n",
            "Non-trainable params: 3,589,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvmlxuTHjJtd",
        "outputId": "23f091f7-494b-4bb7-bb82-5395e56a7592"
      },
      "source": [
        "new_model.evaluate([X_test_padded, x_test_rating, x_test_verified_purchase], y_test, verbose=2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132/132 - 5s - loss: 0.4043 - acc: 0.8374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40432077646255493, 0.8373809456825256]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}